{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run your first RNN for NLP\n",
    "- Get a first taste of what an embedding is\n",
    "\n",
    "<hr>\n",
    "\n",
    "Words are not something you can easily feed to a Neural Network. For this reason, we have to convert them to something more meaningful. \n",
    "\n",
    "And this is exactly what _Embeddings_ are for! They map any word onto a vectorial representation (this a fancy way to represent each word with a vector ;) ). For instance, the word `dog` can be represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, and we will learn the weights $(w_k)_k$.\n",
    "\n",
    "So let's just do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 10:58:40.136736: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/tanushrinayak/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-11-16 10:58:44.394356: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94d5fb395e4463ab8b1ba6ec988cae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6246a4b8094493813649ae0983fd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2d62c761f4aaea5483b5c584942b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb25c27afc34e0e9438a644cb58a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5d23520b75474ead6f02beb0e47a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteHIF67Q/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d81fdb71b8c4834a2e726f38997ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda6331635744ef9910fbcf1439ebdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteHIF67Q/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c872efc5474c009128a2efda291fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9394f391c635450f9cf17dc42226e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteHIF67Q/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "\n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "\n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "\n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded the data, let's check it out!\n",
    "\n",
    "❓ **Question** ❓ You can play with the data here. In particular, `X_train` and `X_test` are lists of sentences. Let's print some of them, with their respective label stored in `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'an', 'absolutely', 'terrible', 'movie', \"don't\", 'be', 'lured', 'in', 'by', 'christopher', 'walken', 'or', 'michael', 'ironside', 'both', 'are', 'great', 'actors', 'but', 'this', 'must', 'simply', 'be', 'their', 'worst', 'role', 'in', 'history', 'even', 'their', 'great', 'acting', 'could', 'not', 'redeem', 'this', \"movie's\", 'ridiculous', 'storyline', 'this', 'movie', 'is', 'an', 'early', 'nineties', 'us', 'propaganda', 'piece', 'the', 'most', 'pathetic', 'scenes', 'were', 'those', 'when', 'the', 'columbian', 'rebels', 'were', 'making', 'their', 'cases', 'for', 'revolutions', 'maria', 'conchita', 'alonso', 'appeared', 'phony', 'and', 'her', 'pseudo', 'love', 'affair', 'with', 'walken', 'was', 'nothing', 'but', 'a', 'pathetic', 'emotional', 'plug', 'in', 'a', 'movie', 'that', 'was', 'devoid', 'of', 'any', 'real', 'meaning', 'i', 'am', 'disappointed', 'that', 'there', 'are', 'movies', 'like', 'this', 'ruining', \"actor's\", 'like', 'christopher', \"walken's\", 'good', 'name', 'i', 'could', 'barely', 'sit', 'through', 'it']\n",
      "0\n",
      "\n",
      "['i', 'have', 'been', 'known', 'to', 'fall', 'asleep', 'during', 'films', 'but', 'this', 'is', 'usually', 'due', 'to', 'a', 'combination', 'of', 'things', 'including', 'really', 'tired', 'being', 'warm', 'and', 'comfortable', 'on', 'the', 'sette', 'and', 'having', 'just', 'eaten', 'a', 'lot', 'however', 'on', 'this', 'occasion', 'i', 'fell', 'asleep', 'because', 'the', 'film', 'was', 'rubbish', 'the', 'plot', 'development', 'was', 'constant', 'constantly', 'slow', 'and', 'boring', 'things', 'seemed', 'to', 'happen', 'but', 'with', 'no', 'explanation', 'of', 'what', 'was', 'causing', 'them', 'or', 'why', 'i', 'admit', 'i', 'may', 'have', 'missed', 'part', 'of', 'the', 'film', 'but', 'i', 'watched', 'the', 'majority', 'of', 'it', 'and', 'everything', 'just', 'seemed', 'to', 'happen', 'of', 'its', 'own', 'accord', 'without', 'any', 'real', 'concern', 'for', 'anything', 'else', 'i', 'cant', 'recommend', 'this', 'film', 'at', 'all']\n",
      "0\n",
      "\n",
      "['mann', 'photographs', 'the', 'alberta', 'rocky', 'mountains', 'in', 'a', 'superb', 'fashion', 'and', 'jimmy', 'stewart', 'and', 'walter', 'brennan', 'give', 'enjoyable', 'performances', 'as', 'they', 'always', 'seem', 'to', 'do', 'br', 'br', 'but', 'come', 'on', 'hollywood', 'a', 'mountie', 'telling', 'the', 'people', 'of', 'dawson', 'city', 'yukon', 'to', 'elect', 'themselves', 'a', 'marshal', 'yes', 'a', 'marshal', 'and', 'to', 'enforce', 'the', 'law', 'themselves', 'then', 'gunfighters', 'battling', 'it', 'out', 'on', 'the', 'streets', 'for', 'control', 'of', 'the', 'town', 'br', 'br', 'nothing', 'even', 'remotely', 'resembling', 'that', 'happened', 'on', 'the', 'canadian', 'side', 'of', 'the', 'border', 'during', 'the', 'klondike', 'gold', 'rush', 'mr', 'mann', 'and', 'company', 'appear', 'to', 'have', 'mistaken', 'dawson', 'city', 'for', 'deadwood', 'the', 'canadian', 'north', 'for', 'the', 'american', 'wild', 'west', 'br', 'br', 'canadian', 'viewers', 'be', 'prepared', 'for', 'a', 'reefer', 'madness', 'type', 'of', 'enjoyable', 'howl', 'with', 'this', 'ludicrous', 'plot', 'or', 'to', 'shake', 'your', 'head', 'in', 'disgust']\n",
      "0\n",
      "\n",
      "['this', 'is', 'the', 'kind', 'of', 'film', 'for', 'a', 'snowy', 'sunday', 'afternoon', 'when', 'the', 'rest', 'of', 'the', 'world', 'can', 'go', 'ahead', 'with', 'its', 'own', 'business', 'as', 'you', 'descend', 'into', 'a', 'big', 'arm', 'chair', 'and', 'mellow', 'for', 'a', 'couple', 'of', 'hours', 'wonderful', 'performances', 'from', 'cher', 'and', 'nicolas', 'cage', 'as', 'always', 'gently', 'row', 'the', 'plot', 'along', 'there', 'are', 'no', 'rapids', 'to', 'cross', 'no', 'dangerous', 'waters', 'just', 'a', 'warm', 'and', 'witty', 'paddle', 'through', 'new', 'york', 'life', 'at', 'its', 'best', 'a', 'family', 'film', 'in', 'every', 'sense', 'and', 'one', 'that', 'deserves', 'the', 'praise', 'it', 'received']\n",
      "1\n",
      "\n",
      "['as', 'others', 'have', 'mentioned', 'all', 'the', 'women', 'that', 'go', 'nude', 'in', 'this', 'film', 'are', 'mostly', 'absolutely', 'gorgeous', 'the', 'plot', 'very', 'ably', 'shows', 'the', 'hypocrisy', 'of', 'the', 'female', 'libido', 'when', 'men', 'are', 'around', 'they', 'want', 'to', 'be', 'pursued', 'but', 'when', 'no', 'men', 'are', 'around', 'they', 'become', 'the', 'pursuers', 'of', 'a', '14', 'year', 'old', 'boy', 'and', 'the', 'boy', 'becomes', 'a', 'man', 'really', 'fast', 'we', 'should', 'all', 'be', 'so', 'lucky', 'at', 'this', 'age', 'he', 'then', 'gets', 'up', 'the', 'courage', 'to', 'pursue', 'his', 'true', 'love']\n",
      "1\n",
      "\n",
      "['this', 'is', 'a', 'film', 'which', 'should', 'be', 'seen', 'by', 'anybody', 'interested', 'in', 'effected', 'by', 'or', 'suffering', 'from', 'an', 'eating', 'disorder', 'it', 'is', 'an', 'amazingly', 'accurate', 'and', 'sensitive', 'portrayal', 'of', 'bulimia', 'in', 'a', 'teenage', 'girl', 'its', 'causes', 'and', 'its', 'symptoms', 'the', 'girl', 'is', 'played', 'by', 'one', 'of', 'the', 'most', 'brilliant', 'young', 'actresses', 'working', 'in', 'cinema', 'today', 'alison', 'lohman', 'who', 'was', 'later', 'so', 'spectacular', 'in', \"'where\", 'the', 'truth', \"lies'\", 'i', 'would', 'recommend', 'that', 'this', 'film', 'be', 'shown', 'in', 'all', 'schools', 'as', 'you', 'will', 'never', 'see', 'a', 'better', 'on', 'this', 'subject', 'alison', 'lohman', 'is', 'absolutely', 'outstanding', 'and', 'one', 'marvels', 'at', 'her', 'ability', 'to', 'convey', 'the', 'anguish', 'of', 'a', 'girl', 'suffering', 'from', 'this', 'compulsive', 'disorder', 'if', 'barometers', 'tell', 'us', 'the', 'air', 'pressure', 'alison', 'lohman', 'tells', 'us', 'the', 'emotional', 'pressure', 'with', 'the', 'same', 'degree', 'of', 'accuracy', 'her', 'emotional', 'range', 'is', 'so', 'precise', 'each', 'scene', 'could', 'be', 'measured', 'microscopically', 'for', 'its', 'gradations', 'of', 'trauma', 'on', 'a', 'scale', 'of', 'rising', 'hysteria', 'and', 'desperation', 'which', 'reaches', 'unbearable', 'intensity', 'mare', 'winningham', 'is', 'the', 'perfect', 'choice', 'to', 'play', 'her', 'mother', 'and', 'does', 'so', 'with', 'immense', 'sympathy', 'and', 'a', 'range', 'of', 'emotions', 'just', 'as', 'finely', 'tuned', 'as', \"lohman's\", 'together', 'they', 'make', 'a', 'pair', 'of', 'sensitive', 'emotional', 'oscillators', 'vibrating', 'in', 'resonance', 'with', 'one', 'another', 'this', 'film', 'is', 'really', 'an', 'astonishing', 'achievement', 'and', 'director', 'katt', 'shea', 'should', 'be', 'proud', 'of', 'it', 'the', 'only', 'reason', 'for', 'not', 'seeing', 'it', 'is', 'if', 'you', 'are', 'not', 'interested', 'in', 'people', 'but', 'even', 'if', 'you', 'like', 'nature', 'films', 'best', 'this', 'is', 'after', 'all', 'animal', 'behaviour', 'at', 'the', 'sharp', 'edge', 'bulimia', 'is', 'an', 'extreme', 'version', 'of', 'how', 'a', 'tormented', 'soul', 'can', 'destroy', 'her', 'own', 'body', 'in', 'a', 'frenzy', 'of', 'despair', 'and', 'if', 'we', \"don't\", 'sympathise', 'with', 'people', 'suffering', 'from', 'the', 'depths', 'of', 'despair', 'then', 'we', 'are', 'dead', 'inside']\n",
      "1\n",
      "\n",
      "['okay', 'you', 'have', 'br', 'br', 'penelope', 'keith', 'as', 'miss', 'herringbone', 'tweed', 'b', 'b', 'e', 'backbone', 'of', 'england', \"she's\", 'killed', 'off', 'in', 'the', 'first', 'scene', \"that's\", 'right', 'folks', 'this', 'show', 'has', 'no', 'backbone', 'br', 'br', 'peter', \"o'toole\", 'as', \"ol'\", 'colonel', 'cricket', 'from', 'the', 'first', 'war', 'and', 'now', 'the', 'emblazered', 'lord', 'of', 'the', 'manor', 'br', 'br', 'joanna', 'lumley', 'as', 'the', 'ensweatered', 'lady', 'of', 'the', 'manor', '20', 'years', 'younger', 'than', 'the', 'colonel', 'and', '20', 'years', 'past', 'her', 'own', 'prime', 'but', 'still', 'glamourous', 'brit', 'spelling', 'not', 'mine', 'enough', 'to', 'have', 'a', 'toy', 'boy', 'on', 'the', 'side', \"it's\", 'alright', 'they', 'have', 'col', \"cricket's\", 'full', 'knowledge', 'and', 'consent', 'they', 'guy', 'even', 'comes', \"'round\", 'for', 'christmas', 'still', \"she's\", 'considerate', 'of', 'the', 'colonel', 'enough', 'to', 'have', 'said', 'toy', 'boy', 'her', 'own', 'age', 'what', 'a', 'gal', 'br', 'br', 'david', 'mccallum', 'as', 'said', 'toy', 'boy', 'equally', 'as', 'pointlessly', 'glamourous', 'as', 'his', 'squeeze', 'pilcher', \"couldn't\", 'come', 'up', 'with', 'any', 'cover', 'for', 'him', 'within', 'the', 'story', 'so', 'she', 'gave', 'him', 'a', 'hush', 'hush', 'job', 'at', 'the', 'circus', 'br', 'br', 'and', 'finally', 'br', 'br', 'susan', 'hampshire', 'as', 'miss', 'polonia', 'teacups', 'venerable', 'headmistress', 'of', 'the', 'venerable', \"girls'\", 'boarding', 'school', 'serving', 'tea', 'in', 'her', 'office', 'with', 'a', 'dash', 'of', 'deep', 'poignant', 'advice', 'for', 'life', 'in', 'the', 'outside', 'world', 'just', 'before', 'graduation', 'her', 'best', 'bit', 'of', 'advice', \"i've\", 'only', 'been', 'to', 'nancherrow', 'the', 'local', 'stately', 'home', 'of', 'england', 'once', 'i', 'thought', 'it', 'was', 'very', 'beautiful', 'but', 'somehow', 'not', 'part', 'of', 'the', 'real', 'world', 'well', 'we', \"can't\", 'say', 'they', \"didn't\", 'warn', 'us', 'br', 'br', 'ah', 'susan', 'time', 'was', 'your', 'character', 'would', 'have', 'been', 'running', 'the', 'whole', 'show', 'they', \"don't\", 'write', \"'em\", 'like', 'that', 'any', 'more', 'our', 'loss', 'not', 'yours', 'br', 'br', 'so', 'with', 'a', 'cast', 'and', 'setting', 'like', 'this', 'you', 'have', 'the', 're', 'makings', 'of', 'brideshead', 'revisited', 'right', 'br', 'br', 'wrong', 'they', 'took', 'these', '1', 'dimensional', 'supporting', 'roles', 'because', 'they', 'paid', 'so', 'well', 'after', 'all', 'acting', 'is', 'one', 'of', 'the', 'oldest', 'temp', 'jobs', 'there', 'is', 'you', 'name', 'another', 'br', 'br', 'first', 'warning', 'sign', 'lots', 'and', 'lots', 'of', 'backlighting', 'they', 'get', 'around', 'it', 'by', 'shooting', 'outdoors', 'hey', \"it's\", 'just', 'the', 'sunlight', 'br', 'br', 'second', 'warning', 'sign', 'leading', 'lady', 'cries', 'a', 'lot', 'when', 'not', 'crying', 'her', 'eyes', 'are', 'moist', \"that's\", 'the', 'law', 'of', 'romance', 'novels', 'leading', 'lady', 'is', 'dewy', 'eyed', 'br', 'br', 'henceforth', 'leading', 'lady', 'shall', 'be', 'known', 'as', 'l', 'l', 'br', 'br', 'third', 'warning', 'sign', 'l', 'l', 'actually', 'has', 'stars', 'in', 'her', 'eyes', 'when', \"she's\", 'in', 'love', 'still', \"i'll\", 'give', 'emily', 'mortimer', 'an', 'award', 'just', 'for', 'having', 'to', 'act', 'with', 'that', 'spotlight', 'in', 'her', 'eyes', 'i', 'wonder', 'did', 'they', 'use', 'contacts', 'br', 'br', 'and', 'lastly', 'fourth', 'warning', 'sign', 'no', 'on', 'screen', 'female', 'character', 'is', 'mrs', \"she's\", 'either', 'miss', 'or', 'lady', 'br', 'br', 'when', 'all', 'was', 'said', 'and', 'done', 'i', 'still', \"couldn't\", 'tell', 'you', 'who', 'was', 'pursuing', 'whom', 'and', 'why', 'i', \"couldn't\", 'even', 'tell', 'you', 'what', 'was', 'said', 'and', 'done', 'br', 'br', 'to', 'sum', 'up', 'they', 'all', 'live', 'through', 'world', 'war', 'ii', 'without', 'anything', 'happening', 'to', 'them', 'at', 'all', 'br', 'br', 'ok', 'at', 'the', 'end', 'l', 'l', 'finds', \"she's\", 'lost', 'her', 'parents', 'to', 'the', 'japanese', 'prison', 'camps', 'and', 'baby', 'sis', 'comes', 'home', 'catatonic', 'meanwhile', \"there's\", 'always', 'a', 'meanwhile', 'some', 'young', 'guy', 'l', 'l', 'had', 'a', 'crush', 'on', 'when', 'i', \"don't\", 'know', 'comes', 'home', 'from', 'some', 'wartime', 'tough', 'spot', 'and', 'is', 'found', 'living', 'on', 'the', 'street', 'by', 'lady', 'of', 'the', 'manor', 'must', 'be', 'some', 'street', 'if', \"she's\", 'going', 'to', 'find', 'him', 'there', 'both', 'war', 'casualties', 'are', 'whisked', 'away', 'to', 'recover', 'at', 'nancherrow', 'somebody', 'has', 'to', 'be', 'whisked', 'away', 'somewhere', 'in', 'these', 'romance', 'stories', 'br', 'br', 'great', 'drama']\n",
      "0\n",
      "\n",
      "['the', 'film', 'is', 'based', 'on', 'a', 'genuine', '1950s', 'novel', 'br', 'br', 'journalist', 'colin', 'mcinnes', 'wrote', 'a', 'set', 'of', 'three', 'london', 'novels', 'absolute', 'beginners', 'city', 'of', 'spades', 'and', 'mr', 'love', 'and', 'justice', 'i', 'have', 'read', 'all', 'three', 'the', 'first', 'two', 'are', 'excellent', 'the', 'last', 'perhaps', 'an', 'experiment', 'that', 'did', 'not', 'come', 'off', 'but', \"mcinnes's\", 'work', 'is', 'highly', 'acclaimed', 'and', 'rightly', 'so', 'this', 'musical', 'is', 'the', \"novelist's\", 'ultimate', 'nightmare', 'to', 'see', 'the', 'fruits', 'of', \"one's\", 'mind', 'being', 'turned', 'into', 'a', 'glitzy', 'badly', 'acted', 'soporific', 'one', 'dimensional', 'apology', 'of', 'a', 'film', 'that', 'says', 'it', 'captures', 'the', 'spirit', 'of', '1950s', 'london', 'and', 'does', 'nothing', 'of', 'the', 'sort', 'br', 'br', 'thank', 'goodness', 'colin', 'mcinnes', \"wasn't\", 'alive', 'to', 'witness', 'it']\n",
      "0\n",
      "\n",
      "['i', 'really', 'love', 'the', 'sexy', 'action', 'and', 'sci', 'fi', 'films', 'of', 'the', 'sixties', 'and', 'its', 'because', 'of', 'the', \"actress's\", 'that', 'appeared', 'in', 'them', 'they', 'found', 'the', 'sexiest', 'women', 'to', 'be', 'in', 'these', 'films', 'and', 'it', \"didn't\", 'matter', 'if', 'they', 'could', 'act', 'remember', 'candy', 'the', 'reason', 'i', 'was', 'disappointed', 'by', 'this', 'film', 'was', 'because', 'it', \"wasn't\", 'nostalgic', 'enough', 'the', 'story', 'here', 'has', 'a', 'european', 'sci', 'fi', 'film', 'called', 'dragonfly', 'being', 'made', 'and', 'the', 'director', 'is', 'fired', 'so', 'the', 'producers', 'decide', 'to', 'let', 'a', 'young', 'aspiring', 'filmmaker', 'jeremy', 'davies', 'to', 'complete', 'the', 'picture', \"they're\", 'is', 'one', 'real', 'beautiful', 'woman', 'in', 'the', 'film', 'who', 'plays', 'dragonfly', 'but', \"she's\", 'barely', 'in', 'it', 'film', 'is', 'written', 'and', 'directed', 'by', 'roman', 'coppola', 'who', 'uses', 'some', 'of', 'his', 'fathers', 'exploits', 'from', 'his', 'early', 'days', 'and', 'puts', 'it', 'into', 'the', 'script', 'i', 'wish', 'the', 'film', 'could', 'have', 'been', 'an', 'homage', 'to', 'those', 'early', 'films', 'they', 'could', 'have', 'lots', 'of', 'cameos', 'by', 'actors', 'who', 'appeared', 'in', 'them', 'there', 'is', 'one', 'actor', 'in', 'this', 'film', 'who', 'was', 'popular', 'from', 'the', 'sixties', 'and', 'its', 'john', 'phillip', 'law', 'barbarella', 'gerard', 'depardieu', 'giancarlo', 'giannini', 'and', 'dean', 'stockwell', 'appear', 'as', 'well', 'i', 'guess', \"i'm\", 'going', 'to', 'have', 'to', 'continue', 'waiting', 'for', 'a', 'director', 'to', 'make', 'a', 'good', 'homage', 'to', 'the', 'films', 'of', 'the', 'sixties', 'if', 'any', 'are', 'reading', 'this', 'make', 'it', 'as', 'sexy', 'as', 'you', 'can', \"i'll\", 'be', 'waiting']\n",
      "0\n",
      "\n",
      "['sure', 'this', 'one', \"isn't\", 'really', 'a', 'blockbuster', 'nor', 'does', 'it', 'target', 'such', 'a', 'position', 'dieter', 'is', 'the', 'first', 'name', 'of', 'a', 'quite', 'popular', 'german', 'musician', 'who', 'is', 'either', 'loved', 'or', 'hated', 'for', 'his', 'kind', 'of', 'acting', 'and', 'thats', 'exactly', 'what', 'this', 'movie', 'is', 'about', 'it', 'is', 'based', 'on', 'the', 'autobiography', 'dieter', 'bohlen', 'wrote', 'a', 'few', 'years', 'ago', 'but', \"isn't\", 'meant', 'to', 'be', 'accurate', 'on', 'that', 'the', 'movie', 'is', 'filled', 'with', 'some', 'sexual', 'offensive', 'content', 'at', 'least', 'for', 'american', 'standard', 'which', 'is', 'either', 'amusing', 'not', 'for', 'the', 'other', 'actors', 'of', 'course', 'or', 'dumb', 'it', 'depends', 'on', 'your', 'individual', 'kind', 'of', 'humor', 'or', 'on', 'you', 'being', 'a', 'bohlen', 'fan', 'or', 'not', 'technically', 'speaking', 'there', \"isn't\", 'much', 'to', 'criticize', 'speaking', 'of', 'me', 'i', 'find', 'this', 'movie', 'to', 'be', 'an', 'ok', 'movie']\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Print the first 10 sentences of the training set, one at a time (use a for loop).\n",
    "# And their labels\n",
    "\n",
    "for i in range(10):\n",
    "    print(X_train[i])\n",
    "    print(y_train[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELS**: the task is a binary classification problem:\n",
    "- label 0️⃣ corresponds to a <font color=red>negative</font> movie review\n",
    "- label 1️⃣ corresponds to a <font color=green>positive</font> movie review\n",
    "\n",
    "**INPUTS**: \n",
    "- 🧹 The data has been partially cleaned! So you don't have to worry about it in this exercise. \n",
    "- ❗️ But don't forget this step in real-life challenges. \n",
    "\n",
    "Remember that words are not computer-compatible materials? You have to tokenize them!\n",
    "\n",
    "❓ **Question** ❓ Run the following cell to tokenize your sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# This initializes a Keras utilities that does all the tokenization for you\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Print some of the tokenized sentences to be sure you got what you expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 13, 33, 422, 402, 18, 91, 27, 6893, 8, 31, 1311, 4504, 39, 539, 8007, 194, 24, 75, 161, 17, 11, 228, 336, 27, 68, 252, 203, 8, 468, 58, 68, 75, 108, 98, 21, 4165, 11, 1418, 694, 695, 11, 18, 6, 33, 389, 6065, 204, 2345, 487, 1, 86, 928, 140, 69, 158, 51, 1, 16730, 8008, 69, 251, 68, 2895, 15, 16731, 1877, 12117, 12118, 1344, 4505, 3, 38, 3640, 120, 1386, 16, 4504, 13, 164, 17, 2, 928, 805, 8009, 8, 2, 18, 12, 13, 3231, 4, 99, 142, 1239, 9, 221, 696, 12, 48, 24, 97, 35, 11, 8010, 4506, 35, 1311, 16732, 49, 375, 9, 98, 1006, 753, 137, 10]\n",
      "\n",
      "[9, 25, 76, 572, 5, 730, 1945, 296, 100, 17, 11, 6, 625, 662, 5, 2, 1835, 4, 190, 555, 61, 1170, 115, 2531, 3, 2896, 20, 1, 9585, 3, 264, 40, 5424, 2, 187, 200, 20, 11, 3435, 9, 1387, 1945, 80, 1, 19, 13, 1601, 1, 103, 1056, 13, 2090, 1646, 515, 3, 319, 190, 443, 5, 531, 17, 16, 53, 1878, 4, 46, 13, 4507, 94, 39, 135, 9, 929, 9, 195, 25, 1007, 170, 4, 1, 19, 17, 9, 262, 1, 2532, 4, 10, 3, 273, 40, 443, 5, 531, 4, 96, 181, 9586, 213, 99, 142, 4508, 15, 240, 298, 9, 2897, 401, 11, 19, 30, 29]\n",
      "\n",
      "[2445, 6894, 1, 12119, 3232, 2898, 8, 2, 1008, 1602, 3, 1879, 904, 3, 1697, 8011, 184, 790, 370, 14, 32, 205, 328, 5, 74, 7, 7, 17, 222, 20, 342, 2, 16733, 1057, 1, 92, 4, 3233, 444, 12120, 5, 12121, 494, 2, 5425, 416, 2, 5425, 3, 5, 16734, 1, 1137, 494, 87, 12122, 6066, 10, 42, 20, 1, 2770, 15, 987, 4, 1, 449, 7, 7, 164, 58, 3056, 5426, 12, 590, 20, 1, 1880, 429, 4, 1, 4925, 296, 1, 16735, 2016, 3234, 390, 2445, 3, 1089, 861, 5, 25, 2648, 3233, 444, 15, 9587, 1, 1880, 2649, 15, 1, 297, 1283, 1312, 7, 7, 1880, 815, 27, 3235, 15, 2, 9588, 3057, 527, 4, 790, 9589, 16, 11, 2446, 103, 39, 5, 3236, 132, 382, 8, 4509]\n",
      "\n",
      "[11, 6, 1, 231, 4, 19, 15, 2, 6895, 2650, 2771, 51, 1, 365, 4, 1, 189, 67, 134, 1345, 16, 96, 181, 988, 14, 22, 12123, 81, 2, 208, 3237, 2772, 3, 12124, 15, 2, 346, 4, 640, 384, 370, 37, 8012, 3, 4926, 1836, 14, 205, 8013, 3436, 1, 103, 362, 48, 24, 53, 12125, 5, 1603, 53, 1946, 4166, 40, 2, 2531, 3, 2249, 16736, 137, 157, 634, 109, 30, 96, 106, 2, 216, 19, 8, 159, 281, 3, 26, 12, 989, 1, 2773, 10, 2651]\n",
      "\n",
      "[14, 423, 25, 1073, 29, 1, 464, 12, 134, 3641, 8, 11, 19, 24, 626, 422, 1555, 1, 103, 52, 6896, 295, 1, 8014, 4, 1, 601, 16737, 51, 372, 24, 185, 32, 173, 5, 27, 9590, 17, 51, 53, 372, 24, 185, 32, 417, 1, 16738, 4, 2, 2346, 310, 153, 373, 3, 1, 373, 506, 2, 122, 61, 705, 77, 136, 29, 27, 34, 1556, 30, 11, 523, 28, 87, 201, 54, 1, 3642, 5, 4927, 23, 277, 120]\n",
      "\n",
      "[11, 6, 2, 19, 59, 136, 27, 113, 31, 1557, 872, 8, 12126, 31, 39, 1744, 37, 33, 1780, 6067, 10, 6, 33, 5427, 2017, 3, 3437, 1036, 4, 12127, 8, 2, 1881, 247, 96, 2652, 3, 96, 16739, 1, 247, 6, 274, 31, 26, 4, 1, 86, 532, 176, 1171, 854, 8, 477, 591, 4510, 6897, 36, 13, 289, 34, 2447, 8, 16740, 1, 1058, 16741, 9, 60, 401, 12, 11, 19, 27, 627, 8, 29, 4928, 14, 22, 83, 112, 66, 2, 124, 20, 11, 1037, 4510, 6897, 6, 422, 1313, 3, 26, 16742, 30, 38, 1284, 5, 2347, 1, 5428, 4, 2, 247, 1744, 37, 11, 6068, 6067, 41, 16743, 395, 204, 1, 798, 4929, 4510, 6897, 791, 204, 1, 805, 4929, 16, 1, 168, 2899, 4, 4167, 38, 805, 2018, 6, 34, 4930, 263, 133, 98, 27, 8015, 16744, 15, 96, 16745, 4, 6898, 20, 2, 1947, 4, 3438, 6899, 3, 3058, 59, 4511, 3439, 2653, 12128, 16746, 6, 1, 349, 1038, 5, 308, 38, 413, 3, 129, 34, 16, 12129, 2348, 3, 2, 2018, 4, 1346, 40, 14, 5429, 6900, 14, 16747, 299, 32, 95, 2, 2533, 4, 3437, 805, 16748, 16749, 8, 9591, 16, 26, 166, 11, 19, 6, 61, 33, 4931, 3238, 3, 160, 16750, 16751, 136, 27, 1781, 4, 10, 1, 62, 282, 15, 21, 320, 10, 6, 41, 22, 24, 21, 872, 8, 92, 17, 58, 41, 22, 35, 799, 100, 106, 11, 6, 104, 29, 1263, 3883, 30, 1, 2250, 1388, 12127, 6, 33, 1347, 313, 4, 84, 2, 4168, 1192, 67, 1782, 38, 181, 635, 8, 2, 8016, 4, 3239, 3, 41, 77, 91, 8017, 16, 92, 1744, 37, 1, 6069, 4, 3239, 87, 77, 24, 351, 1240]\n",
      "\n",
      "[1018, 22, 25, 7, 7, 9592, 4512, 14, 654, 16752, 12130, 533, 533, 1019, 6901, 4, 2168, 480, 580, 125, 8, 1, 78, 133, 188, 209, 1348, 11, 123, 45, 53, 6901, 7, 7, 731, 12131, 14, 6070, 2349, 12132, 37, 1, 78, 325, 3, 144, 1, 16753, 1745, 4, 1, 8018, 7, 7, 8019, 12133, 14, 1, 16754, 592, 4, 1, 8018, 953, 162, 1123, 71, 1, 2349, 3, 953, 162, 524, 38, 181, 2251, 17, 128, 12134, 5430, 5431, 21, 2350, 191, 5, 25, 2, 3059, 373, 20, 1, 429, 44, 2448, 32, 25, 9593, 16755, 385, 1604, 3, 6902, 32, 235, 58, 270, 16756, 15, 780, 128, 480, 12135, 4, 1, 2349, 191, 5, 25, 307, 3059, 373, 38, 181, 523, 46, 2, 4513, 7, 7, 568, 16757, 14, 307, 3059, 373, 1241, 14, 12136, 12134, 14, 23, 9594, 16758, 354, 222, 54, 16, 99, 1264, 15, 89, 806, 1, 64, 34, 55, 561, 89, 2, 4514, 4514, 292, 30, 1, 4169, 7, 7, 3, 376, 7, 7, 2252, 16759, 14, 654, 16760, 16761, 12137, 16762, 4, 1, 12137, 8020, 6071, 399, 4170, 2654, 8, 38, 1090, 16, 2, 8021, 4, 1074, 3240, 1882, 15, 109, 8, 1, 1059, 189, 40, 149, 8022, 38, 106, 179, 4, 1882, 214, 62, 76, 5, 12138, 1, 663, 12139, 314, 4, 2168, 294, 9, 177, 10, 13, 52, 326, 17, 754, 21, 170, 4, 1, 142, 189, 70, 77, 183, 130, 32, 150, 2449, 204, 7, 7, 4515, 2252, 57, 13, 132, 110, 60, 25, 76, 636, 1, 218, 123, 32, 91, 967, 3643, 35, 12, 99, 50, 306, 1883, 21, 6072, 7, 7, 34, 16, 2, 182, 3, 862, 35, 11, 22, 25, 1, 800, 9595, 4, 12140, 12141, 209, 7, 7, 363, 32, 581, 119, 290, 2253, 706, 612, 80, 32, 1242, 34, 70, 104, 29, 108, 6, 26, 4, 1, 3884, 12142, 2774, 48, 6, 22, 375, 166, 7, 7, 78, 1349, 1419, 732, 3, 732, 4, 16763, 32, 73, 185, 10, 31, 1091, 12143, 1243, 44, 40, 1, 8023, 7, 7, 293, 1349, 1419, 839, 592, 3644, 2, 187, 51, 21, 2534, 38, 498, 24, 12144, 188, 1, 1137, 4, 954, 2351, 839, 592, 6, 12145, 2655, 7, 7, 9596, 839, 592, 3440, 27, 572, 14, 1783, 1783, 7, 7, 773, 1349, 1419, 1783, 1783, 165, 45, 391, 8, 38, 498, 51, 480, 8, 120, 128, 573, 184, 3241, 12146, 33, 1605, 40, 15, 264, 5, 469, 16, 12, 12147, 8, 38, 498, 9, 574, 116, 32, 418, 6073, 7, 7, 3, 9597, 3441, 1349, 1419, 53, 20, 256, 601, 110, 6, 1698, 480, 339, 654, 39, 592, 7, 7, 51, 29, 13, 307, 3, 196, 9, 128, 354, 395, 22, 36, 13, 8024, 781, 3, 135, 9, 354, 58, 395, 22, 46, 13, 307, 3, 196, 7, 7, 5, 2656, 54, 32, 29, 437, 137, 189, 325, 1520, 213, 240, 1209, 5, 94, 30, 29, 7, 7, 543, 30, 1, 121, 1783, 1783, 714, 480, 441, 38, 822, 5, 1, 715, 1210, 9598, 3, 792, 16764, 270, 314, 16765, 1746, 224, 205, 2, 1746, 47, 176, 235, 1783, 1783, 63, 2, 3645, 20, 51, 9, 91, 127, 270, 314, 37, 47, 6903, 1211, 1699, 3, 6, 260, 761, 20, 1, 762, 31, 592, 4, 1, 8018, 228, 27, 47, 762, 41, 480, 163, 5, 148, 89, 48, 194, 325, 12148, 24, 8025, 232, 5, 6904, 30, 12138, 1389, 45, 5, 27, 8025, 232, 1314, 8, 119, 954, 664, 7, 7, 75, 396]\n",
      "\n",
      "[1, 19, 6, 405, 20, 2, 1606, 4516, 613, 7, 7, 2535, 6074, 12149, 840, 2, 265, 4, 246, 1092, 2351, 1265, 12150, 444, 4, 9599, 3, 390, 120, 3, 1390, 9, 25, 311, 29, 246, 1, 78, 105, 24, 335, 1, 242, 366, 33, 2091, 12, 116, 21, 222, 125, 17, 16766, 169, 6, 500, 6075, 3, 8026, 34, 11, 617, 6, 1, 12151, 1784, 1647, 5, 66, 1, 12152, 4, 1648, 334, 115, 582, 81, 2, 12153, 915, 1009, 16767, 26, 2253, 12154, 4, 2, 19, 12, 516, 10, 2657, 1, 1113, 4, 4516, 1092, 3, 129, 164, 4, 1, 406, 7, 7, 1391, 3442, 6074, 12149, 275, 1350, 5, 2536, 10]\n",
      "\n",
      "[9, 61, 120, 1, 1315, 226, 3, 930, 931, 100, 4, 1, 3443, 3, 96, 80, 4, 1, 16768, 12, 1344, 8, 94, 32, 260, 1, 9600, 464, 5, 27, 8, 119, 100, 3, 10, 150, 535, 41, 32, 98, 469, 453, 2019, 1, 282, 9, 13, 696, 31, 11, 19, 13, 80, 10, 275, 3060, 191, 1, 64, 139, 45, 2, 1484, 930, 931, 19, 425, 9601, 115, 90, 3, 1, 160, 6, 4517, 34, 1, 1266, 990, 5, 392, 2, 176, 6076, 1448, 4932, 3646, 5, 697, 1, 450, 430, 6, 26, 142, 326, 255, 8, 1, 19, 36, 303, 9601, 17, 480, 1006, 8, 10, 19, 6, 398, 3, 545, 31, 6077, 9602, 36, 1075, 47, 4, 23, 4518, 8027, 37, 23, 389, 431, 3, 1316, 10, 81, 1, 227, 9, 583, 1, 19, 98, 25, 76, 33, 4519, 5, 158, 389, 100, 32, 98, 25, 732, 4, 2537, 31, 161, 36, 1344, 8, 94, 48, 6, 26, 243, 8, 11, 19, 36, 13, 1039, 37, 1, 3443, 3, 96, 337, 6078, 1137, 12155, 6905, 9603, 9604, 12156, 3, 3242, 12157, 861, 14, 70, 9, 528, 146, 163, 5, 25, 5, 1485, 905, 15, 2, 160, 5, 95, 2, 49, 4519, 5, 1, 100, 4, 1, 3443, 41, 99, 24, 1040, 11, 95, 10, 14, 1315, 14, 22, 67, 573, 27, 905]\n",
      "\n",
      "[244, 11, 26, 215, 61, 2, 2900, 716, 129, 10, 2092, 138, 2, 2538, 12158, 6, 1, 78, 375, 4, 2, 174, 1039, 1076, 4933, 36, 6, 339, 419, 39, 1884, 15, 23, 231, 4, 108, 3, 1948, 733, 46, 11, 18, 6, 43, 10, 6, 405, 20, 1, 8028, 12158, 12159, 840, 2, 175, 162, 598, 17, 215, 884, 5, 27, 2017, 20, 12, 1, 18, 6, 1392, 16, 47, 991, 2254, 1649, 30, 223, 15, 297, 1226, 59, 6, 339, 1020, 21, 15, 1, 82, 161, 4, 261, 39, 992, 10, 4520, 20, 132, 2169, 231, 4, 495, 39, 20, 22, 115, 2, 12159, 304, 39, 21, 2352, 1449, 48, 215, 72, 5, 5432, 1449, 4, 65, 9, 148, 11, 18, 5, 27, 33, 543, 18]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Print some of the tokenized sentences of the training set, one at a time (use a for loop).\n",
    "\n",
    "for i in range(10):\n",
    "    print(X_train_token[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary that maps each word to a token can be accessed with `tokenizer.word_index`\n",
    "    \n",
    "❓ **Question** ❓ Add a `vocab_size` variable that stores the number of different words (=tokens) in the train set. This is called the _size of the vocabulary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30420\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Add a vocab_size variable that contains the size of the vocabulary (the number of unique words in the train set)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `X_train_token` and `X_test_token` contain sequences of different lengths.\n",
    "\n",
    "<img src=\"padding.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "However, a neural network has to have a tensor as input. For this reason, you have to pad your data.\n",
    "\n",
    "❓ **Question** ❓  Pad your data with the `pad_sequences` function (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)). Do not forget about the `dtype` and `padding` keywords (but do not use `maxlen` here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 13, 33, 422, 402, 18, 91, 27, 6893, 8, 31, 1311, 4504, 39, 539, 8007, 194, 24, 75, 161, 17, 11, 228, 336, 27, 68, 252, 203, 8, 468, 58, 68, 75, 108, 98, 21, 4165, 11, 1418, 694, 695, 11, 18, 6, 33, 389, 6065, 204, 2345, 487, 1, 86, 928, 140, 69, 158, 51, 1, 16730, 8008, 69, 251, 68, 2895, 15, 16731, 1877, 12117, 12118, 1344, 4505, 3, 38, 3640, 120, 1386, 16, 4504, 13, 164, 17, 2, 928, 805, 8009, 8, 2, 18, 12, 13, 3231, 4, 99, 142, 1239, 9, 221, 696, 12, 48, 24, 97, 35, 11, 8010, 4506, 35, 1311, 16732, 49, 375, 9, 98, 1006, 753, 137, 10]\n",
      "[  0.   0.   0. ... 753. 137.  10.]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Pad the train and test set to have the same length (use the pad_sequences function from Keras)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_pad = pad_sequences(X_train_token, dtype='float32', padding='pre')\n",
    "\n",
    "# Print the first sentence of the train set, before and after padding\n",
    "\n",
    "print(X_train_token[0])\n",
    "print(X_pad[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now feed this data to a Recurrent Neural Network.\n",
    "\n",
    "❓ **Question** ❓ Write a model that has:\n",
    "- an embedding layer whose `input_dim` is the size of your vocabulary (= your `vocab_size`), and whose `output_dim` is the size of the embedding space you want to have\n",
    "- a RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- a Dense layer\n",
    "- an output layer\n",
    "\n",
    "⚠️ **Warning** ⚠️ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly, which you have to set with `mask_zero=True`. That also means that your data **HAS TO** be padded with **0** (which is the default behavior). See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) to understand how it **impacts** the `input_dim`.\n",
    "\n",
    "<details>\n",
    "    <summary>💡 Hint</summary>\n",
    "\n",
    "`input_dim` should equal size of vocabulary + 1\n",
    "\n",
    "</details>\n",
    "\n",
    "Compile it with the appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a model with\n",
    "# an Embedding layer whose input_dim is the vocab_size variable,\n",
    "# and whose output_dim is the embedding_size variable\n",
    "# a RNN layer (SimpleRNN or LSTM or GRU) with 32 units\n",
    "# a Dense layer and\n",
    "# a final sigmoid layer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "embedding_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size))\n",
    "\n",
    "model.add(SimpleRNN(units=32))\n",
    "\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Look at the number of parameters in your RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          973440    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 976065 (3.72 MB)\n",
      "Trainable params: 976065 (3.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Double-check that the number of parameters in your embedding layer is equal to the (number of words in your vocabulary + 1 for the masking value) $\\times$  the dimension of your embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The number of parameters of the Embedding layer is vocab_size * embedding_size.\\n\\nThe Embedding layer is a lookup table that maps each word to a vector of size embedding_size.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# What is the number of parameters of the Embedding layer?\n",
    "\n",
    "\"\"\"The number of parameters of the Embedding layer is vocab_size * embedding_size.\n",
    "\n",
    "The Embedding layer is a lookup table that maps each word to a vector of size embedding_size.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Start fitting your model with 20 epochs, with an early stopping criterion whose patience is equal to 4.\n",
    "\n",
    "⚠️ **Warning** ⚠️ You might see that it takes a lot of time! \n",
    "\n",
    "**So stop it after a couple of iterations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 12s 177ms/step - loss: 0.6947 - accuracy: 0.5105 - val_loss: 0.6953 - val_accuracy: 0.4860\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 11s 174ms/step - loss: 0.6362 - accuracy: 0.7560 - val_loss: 0.6776 - val_accuracy: 0.5500\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 11s 176ms/step - loss: 0.3471 - accuracy: 0.9370 - val_loss: 0.6067 - val_accuracy: 0.6600\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 11s 176ms/step - loss: 0.0525 - accuracy: 0.9960 - val_loss: 0.8034 - val_accuracy: 0.6080\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 11s 175ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.8414 - val_accuracy: 0.6380\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 11s 179ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.9396 - val_accuracy: 0.6360\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 11s 172ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.0178 - val_accuracy: 0.6280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17b95ed40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Fit the model with 20 epochs with early stopping and patience of 4\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=4)\n",
    "\n",
    "model.fit(X_pad, y_train, epochs=20, validation_split=0.2,\n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not waste too much time just staring at our screen or having coffees. It is too early to start having breaks ;)\n",
    "\n",
    "❓ **Question** ❓ We will reduce the computational time. To start, let's first look at how many words there are in the different sentences of your train set (Just run the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_hist(X):\n",
    "    len_ = [len(_) for _ in X]\n",
    "    plt.hist(len_)\n",
    "    plt.title('Histogram of the number of sentences that have a given number of words')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably see that 90 to 95% of your sentences have less than 300 words. And very few have more than 1000.\n",
    "\n",
    "However, as you didn't use `maxlen` in your padding above, your input tensor has a dimension equal to the length of the sentence that has the maximum number of words.\n",
    "\n",
    "Now, let's look at how this affects the padding: \n",
    "\n",
    "\n",
    "<img src=\"tensor_size.png\" alt='Word2Vec' width=\"700px\" />\n",
    "\n",
    "Because of a few of very long sentences, one dimension of your tensor is equal to around 1000. However, most of the sentences with ~200 words have just padded values that are useless.\n",
    "\n",
    "So your tensor is mostly useless information, which still adds time to the training process.\n",
    "\n",
    "But what if you pad the data to a maximum length (`maxlen`) of say 200 (words)?\n",
    "- First, that would increase the convergence and you would not need to stare at your screen while waiting for the algorithm to converge\n",
    "- But in essence, do you really lose that much information? Do you think that you often need more than 200 words (up to 1000) to tell whether or not a sentence is positive of negative?\n",
    "\n",
    "❓ **Question** ❓ For all these reasons, re-do your padding using the `maxlen` keyword and retrain the model!  See how much faster it is now - without hurting the performance ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 3s 36ms/step - loss: 0.6982 - accuracy: 0.4775 - val_loss: 0.6898 - val_accuracy: 0.5300\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.6339 - accuracy: 0.6795 - val_loss: 0.6813 - val_accuracy: 0.5740\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 2s 35ms/step - loss: 0.3563 - accuracy: 0.8835 - val_loss: 0.7241 - val_accuracy: 0.5780\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 0.0602 - accuracy: 0.9950 - val_loss: 0.9792 - val_accuracy: 0.5720\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 1.0990 - val_accuracy: 0.5800\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.1970 - val_accuracy: 0.5840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x181c9db40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Re-train the model with maxlen = 200\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_pad = pad_sequences(X_train_token, dtype=\"float32\",\n",
    "                      padding=\"pre\", maxlen=maxlen)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size))\n",
    "\n",
    "model.add(SimpleRNN(units=32))\n",
    "\n",
    "model.add(Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=4)\n",
    "\n",
    "model.fit(X_pad, y_train, epochs=20, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏁 Nice, you are now able to use `Tokenizer` and `pad_sequences`\n",
    "\n",
    "💾 Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "🚀 ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
